{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adde86ac",
   "metadata": {},
   "source": [
    "# stats for biology with python\n",
    "In this course, we go through the principles of statistics and machine learning based on the book titled *Python Programming for Biology*: https://www.cambridge.org/core/books/python-programming-for-biology/61762A9F672FDD8B2DD3FFF8773027B2. We focus on the following chapters:\n",
    "\n",
    "1. Chapter 22, Statistics\n",
    "2. Chapter 23, Clustering and discrimination\n",
    "3. Chapter 24, Machine learning\n",
    "\n",
    "Presenter: Dr Ali Fahmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816b57d1",
   "metadata": {},
   "source": [
    "## 1- Statistics\n",
    "### 1-1- Statistical analyses\n",
    "#### Samples and significance\n",
    "A key idea of statistical analyses is that the data we collect contains a limited number of **samples** from some kind of underlying probability\n",
    "distribution. This simplifies the data but allows forming mathematical models to test hypotheses and draw conclusions. For this, we would look at the observed data to estimate the parameters of the distribution. The answer to parameter estimation is true with a certain probability, often called a **confidence level**. \n",
    "\n",
    "The probability distribution is often used for **prediction**, i.e., a model of what we expect at **random** and the competing hypothesis would mean that something significantly non-random was happening.\n",
    "\n",
    "Here, we define a normal distribution with a certain mean, standard deviation, and number of samples. The random distribution is built with the NumPy library. More details: https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from numpy import random\n",
    "mean = 0.0\n",
    "stdDev = 1.0\n",
    "for nPoints in (10, 100, 1000, 10000, 100000):\n",
    "    sample = random.normal(mean, stdDev, nPoints)\n",
    "    pyplot.hist(sample, bins=20, range=(-4,4))#, normed=True) #fixed the code!\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351db7d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create histogram with density curve overlaid\n",
    "import seaborn as sns\n",
    "for nPoints in (10, 100, 1000, 10000,100000):\n",
    "    sample = random.normal(mean, stdDev, nPoints)\n",
    "    g = sns.displot(sample, kde=True, bins=20, kde_kws={\"clip\": [-4, 4]}, height=4, aspect=1.35)\n",
    "    g.set(xlim=(-4, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf53d2",
   "metadata": {},
   "source": [
    "### 1-2- Simple statistical parameters\n",
    "#### Mode\n",
    "The mode is the most commonly occurring value in a set of data. We can find the mode using the following codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create a set called values\n",
    "values = [1,2,2,3,2,1,4,2,3,1,0]\n",
    "#find mode\n",
    "counts = [(values.count(val), val) for val in set(values)]\n",
    "count, mode = max(counts)\n",
    "print('Mode =', mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fc3fd",
   "metadata": {},
   "source": [
    "Another way of finding the mode using the SciPy library. For details, see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mode.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33209dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import array\n",
    "#convert values set into an array\n",
    "valArray = array(values, float) #instead, they can be int (integer)!\n",
    "#find mode\n",
    "mode, count = stats.mode(valArray)\n",
    "print('Mode =', mode[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9674e3dd",
   "metadata": {},
   "source": [
    "#### Median\n",
    "The median represents the middle-ranked value when the data is placed in its sorted order. We can find the median using NumPy library. For details, see: https://numpy.org/doc/stable/reference/generated/numpy.median.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97555ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import median\n",
    "med = median(valArray)\n",
    "print('Median =', med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d6448",
   "metadata": {},
   "source": [
    "Another way of finding the median is by defining a function, instead of using a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d375bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMedian(values):\n",
    "    vSorted = sorted(values)\n",
    "    nValues = len(values) #count of the values\n",
    "    if nValues % 2 == 0: #even count\n",
    "        index = nValues//2 #floor division\n",
    "        median = sum(vSorted[index-1:index+1])/2.0\n",
    "    else: #odd count\n",
    "        index = (nValues-1)//2\n",
    "        median = vSorted[index]\n",
    "    return median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f088c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use getMedian function\n",
    "med = getMedian(values)\n",
    "print('Median =', med)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e6257",
   "metadata": {},
   "source": [
    "#### Mean\n",
    "The mean is the numerical average of a set of values. We can calculate the mean as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(values)/float(len(values))\n",
    "print('Mean =', mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578af6c5",
   "metadata": {},
   "source": [
    "We can round the mean in printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3eb21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean = %.2f' %mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd6cf6",
   "metadata": {},
   "source": [
    "Or we can use the following function to round the mean forever! For details, see: https://docs.python.org/3/library/functions.html#round. Consider defining a new variable (e.g., mean_rounded) in this kind of situation, so that the original mean value would not be missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4381cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rounded = round(mean, 2)\n",
    "print('Rounded mean =', mean_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9cca04",
   "metadata": {},
   "source": [
    "Another way of finding the median is by defining a function, instead of using the NumPy library. For details, see: https://numpy.org/doc/stable/reference/generated/numpy.mean.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6abdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, mean\n",
    "valArray = array(values, float)\n",
    "m = valArray.mean()\n",
    "# or\n",
    "m = mean(valArray)\n",
    "print('Mean =', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4cbc8",
   "metadata": {},
   "source": [
    "#### Variance\n",
    "The variance is a measure of how far the values are spread from the mean, i.e., the expectation of the squared differences from the mean. Variance of a population is shown as $\\sigma^2$, whereas the variance of a sample is shown as $s^2$. An **unbiased estimation** of the underlying variance is as follows:\n",
    "\n",
    "$$\n",
    "  s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_{i}-\\overline{x})^2\n",
    "$$\n",
    "\n",
    "The equation can be simplified for large sample (**biased estimation**), as shown below:\n",
    "\n",
    "$$\n",
    "  s^2 = \\frac{1}{n} \\sum_{i=1}^n (x_{i}-\\overline{x})^2\n",
    "$$\n",
    "\n",
    "We can calculate the variance using the following codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = float(len(values))\n",
    "mean = sum(values)/n\n",
    "diffs = [v-mean for v in values]\n",
    "#unbiased estimate of variance (since it divides by n-1)\n",
    "variance = sum([d*d for d in diffs])/(n-1) \n",
    "print('Variance = %.3f' %variance) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c084bbe",
   "metadata": {},
   "source": [
    "Variance can be calculated using NumPy as shown below. It has a parameter, called ddof, standign for Delta Degrees of Freedom that is the divisor used in the calculation. If ddof is given 1, it generates unbiased variance by calculating *n-1*, otherwise it calculates biased variance since ddof is 0 by default. For details, see: https://numpy.org/doc/stable/reference/generated/numpy.var.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb530170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "valArray = array(values)\n",
    "variance = valArray.var()\n",
    "print('Biased variance = %.3f' %variance)\n",
    "#unbiased estimate. ddof stands for Delta Degrees of Freedom\n",
    "variance = valArray.var(ddof=1)\n",
    "print('Unbiased variance = %.3f' %variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f90924",
   "metadata": {},
   "source": [
    "#### Standard deviation\n",
    "\n",
    "Standard deviation is the square root of the variance. Given the variance, standard deviation can be calculated using NumPy, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "stdDev = sqrt(variance)\n",
    "print('Standard deviation = %.3f' %stdDev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb55cb",
   "metadata": {},
   "source": [
    "Or it can be calculated with a specific function of the NumPy, as shown below. For details, see: https://numpy.org/doc/stable/reference/generated/numpy.std.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e6365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import std\n",
    "stdDev = std(valArray)\n",
    "stdDev = valArray.std(ddof=1)\n",
    "print('Standard deviation = %.3f' %stdDev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b89dd61",
   "metadata": {},
   "source": [
    "#### Standard error\n",
    "Standard error of the mean is the standard deviation in x (i.e., $s_x$) divided by the square root of the number of values.\n",
    "\n",
    "$$\n",
    "  SE_{\\overline{x}} = \\frac{s_x}{\\sqrt{n}}\n",
    "$$\n",
    "\n",
    "It can be calculated using SciPy, as shown below. For details, see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.sem.html#scipy.stats.sem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdErrMean = valArray.std(ddof=1)/sqrt(len(valArray))\n",
    "from scipy.stats import sem\n",
    "stdErrMean = sem(valArray, ddof=1)\n",
    "print('Standard error = %.3f' %stdErrMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085ca95",
   "metadata": {},
   "source": [
    "#### Skewness\n",
    "\n",
    "The skewness of a distribution is a measure of asymmetry or lopsidedness. It is commonly estimated for a sample as the mean cubed difference of the data from the mean divided by the standard deviation cubed.\n",
    "\n",
    "$$\n",
    "  g = \\sum_{i=1}^n \\left(\\frac{x_{i}-\\overline{x}}{s}\\right)^3\n",
    "$$\n",
    "\n",
    "Skewness can be calculated using SciPy library, as shown below. For details, see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skew.html#scipy.stats.skew. Skewness of a Gamma distribution is calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dec6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew\n",
    "from numpy import random\n",
    "samples = random.gamma(3.0, 2.0, 100)\n",
    "skewdness = skew(samples)\n",
    "print('Skewness = %.3f' %skewdness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d0d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean, median\n",
    "samples_mean = mean(samples)\n",
    "print('Mean = %.3f' %samples_mean)\n",
    "samples_median = median(samples)\n",
    "print('Median = %.3f' %samples_median)\n",
    "import seaborn as sns\n",
    "g = sns.displot(samples, kde=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0617fa84",
   "metadata": {},
   "source": [
    "The mean is greater than the median of the samples; so, it is called right-skewed (or positive skewness)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325b50e",
   "metadata": {},
   "source": [
    "### 1-3- Statistical tests\n",
    "\n",
    "Statistical tests estimate the probability of an event occurring, given a hypothesis. A simple test to perform with a probability distribution is to evaluate the likelihood that a value was generated by the distribution. Hence, rather than thinking about the likelihood of a single value, we instead tend to test the probability of a value falling within a given range. This can be formulated as the probability mass function, i.e., the area of a region under the curve defining the probabilities for each value.\n",
    "\n",
    "#### Significance and hypotheses\n",
    "#### Binomial test\n",
    "Binomial test is concerned with the number of occurrences of an event that has a fixed probability of occurring, given a certain number of trials. Bionial test can be conducted using SciPy as shown below. For details, see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom_test.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom_test\n",
    "count_success, nTrials, pEvent = 530, 1000, 0.5\n",
    "result = binom_test(count_success, nTrials, pEvent)\n",
    "print('Probability of success of binomial two tail =', result) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14e5569",
   "metadata": {},
   "source": [
    "#### Z-scores\n",
    "A Z-score (also called the standard score) is the number of standard deviations an observed value is different from the mean.\n",
    "\n",
    "$$\n",
    "  z = \\frac{x-\\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27050a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, abs\n",
    "mean = 1.76\n",
    "stdDev = 0.075\n",
    "values = array([1.8, 1.9, 2.0])\n",
    "zScores = abs(values - mean)/stdDev\n",
    "print('Z scores = ', zScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9b0f5",
   "metadata": {},
   "source": [
    "Z-scores can be calculated using the SciPy library, as shown below. For details, see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.zscore.html#scipy.stats.mstats.zscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ed891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore, norm\n",
    "samples = norm.rvs(mean, stdDev, size=3) # Values for testing\n",
    "zScores = zscore(samples, ddof=1) # Unbiased estimators\n",
    "print('Samples = ', samples)\n",
    "print('Estimated Z scores = ', zScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315b4c8",
   "metadata": {},
   "source": [
    "SciPy operates differently because it estimates its own sample mean and sample standard deviation from the input values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129c770",
   "metadata": {},
   "source": [
    "#### Z-test\n",
    "A related concept to this is the Z-test, which can be used when we have samples that are taken from a normal distribution where the true mean and standard deviation are known. The Z-test is effectively the calculation of a Z-score for a sample mean. We want to know whether the sample is significantly different and the null\n",
    "hypothesis would be that the two populations have the same mean. Z-score for Z-test is defined as below:\n",
    "\n",
    "$$\n",
    "  z = \\frac{\\overline{x}-\\mu_{0}}{\\frac{\\sigma}{\\sqrt{n}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45e3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt\n",
    "from scipy.special import erf\n",
    "def zTestMean(sMean, nSamples, normMean, stdDev, oneSided=True):\n",
    "    zScore = abs(sMean - normMean) / (stdDev / sqrt(nSamples))\n",
    "    prob = 1-erf(zScore/sqrt(2)) #error function to calculate the cumulative probability\n",
    "    if oneSided:\n",
    "        prob *= 0.5\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = array([1.752, 1.818, 1.597, 1.697, 1.644, 1.593, 1.878, 1.648, 1.819, 1.794, 1.745, 1.827])\n",
    "mean = 1.76\n",
    "stDev = 0.075\n",
    "result = zTestMean(samples.mean(), len(samples), mean, stdDev, oneSided=True)\n",
    "print('Mean of Z-test =', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92966b",
   "metadata": {},
   "source": [
    "The resulting probability of the sample mean coming from the normal distribution is 11.8%, so we generally wouldn’t want to reject the notion that the samples were generated from it. In other words, the samples had no sigificantly different mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a521b1",
   "metadata": {},
   "source": [
    "#### T-tests\n",
    "T-tests are like Z-tests, but will be used if we do not know the underlying mean and standard deviations of the probability distributions. The important difference compared to the Z-score is that the sample standard deviation (i.e., $s$) is used.\n",
    "\n",
    "$$\n",
    "  t = \\frac{\\overline{x}-\\mu_{x}}{\\frac{s}{\\sqrt{n}}}\n",
    "$$\n",
    "\n",
    "Note that there are different formulations of T-statistics that can be used, depending on the question being asked.\n",
    "\n",
    "T-test can be used to find the probability of a sample mean being the same as the true mean from a distribution (e.g., a null hypothesis). For this, t-statistics and the two-tailed probability can be calculated using SciPy, as shown below. More details:\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.ttest_1samp.html#scipy.stats.mstats.ttest_1samp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa697000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "trueMean = 1.76\n",
    "samples = array([1.752, 1.818, 1.597, 1.697, 1.644, 1.593, 1.878, 1.648, 1.819, 1.794, 1.745, 1.827])\n",
    "tStat, twoTailProb = ttest_1samp(samples, trueMean)\n",
    "print('T-statistic and two-tailed probability = ', tStat, twoTailProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f718dc",
   "metadata": {},
   "source": [
    "A T-test can be applied to determine whether two sets of samples, each from a normal distribution, have the same underlying mean, assuming that they have same variance. For this, t-statistics and the two-tailed probability can be calculated using SciPy, as shown below. More details: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mstats.ttest_ind.html#scipy.stats.mstats.ttest_ind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ea2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "samples1 = array([1.752, 1.818, 1.597, 1.697, 1.644, 1.593])\n",
    "samples2 = array([1.878, 1.648, 1.819, 1.794, 1.745, 1.827])\n",
    "tStat, twoTailProb = ttest_ind(samples1, samples2)\n",
    "print('T-statistic and two-tailed probability = ', tStat, twoTailProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa4599",
   "metadata": {},
   "source": [
    "### 1-4- Correlation and covariance\n",
    "#### Covariance\n",
    "Covariance is a measure of whether two random variables vary simultaneously as their values increase or decrease. Hence for two probability distributions, described by random variables X and Y with sample points xi and yi respectively, the covariance may be written as:\n",
    "\n",
    "$$\n",
    "  \\sigma_{xy} = \\sum_{k=1}^n \\frac{(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{N}\n",
    "$$\n",
    "\n",
    "Covariance can be calculated using NumPy library. Details: https://numpy.org/doc/stable/reference/generated/numpy.cov.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e0e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random, cov\n",
    "xVals = random.normal(0.0, 1.0, 100)\n",
    "yVals1 = random.normal(0.0, 1.0, 100) # Random, independent of xVals\n",
    "deltas = random.normal(0.0, 0.75, 100)\n",
    "yVals2 = 0.5 + 2.0 * (xVals - deltas) # Derived from xVals\n",
    "cov1 = cov(xVals, yVals1)\n",
    "cov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov2 = cov(xVals, yVals2)\n",
    "cov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33660099",
   "metadata": {},
   "source": [
    "This difference is because the covalance matrix shows:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "\\sigma_{x}^2 & \\sigma_{xy}\\\\ \n",
    "\\sigma_{yx} & \\sigma_{y}^2\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "So the diagonal is simply the variances for X and Y and the other values are equal to the covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca5e62",
   "metadata": {},
   "source": [
    "#### Correlation coefficient\n",
    "Pearson’s correlation coefficient gives us a handy measure of how well two quantities are linearly correlated:\n",
    "\n",
    "$$\n",
    "  r = \\frac{\\sigma_{xy}}{\\sigma_{x}\\sigma_{y}}\n",
    "$$\n",
    "\n",
    "This correlation can be calculated using NumPy. Details: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import corrcoef\n",
    "r1 = corrcoef(xVals, yVals1)[0, 1]\n",
    "r2 = corrcoef(xVals, yVals2)[0, 1]\n",
    "print('Correlation between xVals and yVals1 = ', r1)\n",
    "print('Correlation between xVals and yVals2 = ', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53111d32",
   "metadata": {},
   "source": [
    "#### Simple linear regression\n",
    "\n",
    "Pearson’s correlation coefficient provides a handy way of doing a line fit called simple linear regression. The gradient of the line can be calculated by dividing the covariance of X and Y by the variance of X.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf71d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import cov, var, mean, random\n",
    "xVals = random.normal(0.0, 1.0, 100)\n",
    "yVals = 2.0 + -0.7 * xVals + random.normal(0.0, 0.2, 100)\n",
    "grad = cov(xVals, yVals)/var(xVals, ddof=1)\n",
    "yInt = mean(yVals) - grad * mean(xVals)\n",
    "print('LR 1:', grad[0, 1], yInt[0, 1]) # fixed the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f302710",
   "metadata": {},
   "source": [
    "We can see that the fitted line has a gradient and intercept close to the artificial test values of -0.7 and 2.0. Parameters of the linear regression can be calculated using SciPy. Details: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbccfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "linregress(xVals, yVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xValsFit = [xVals.min(),xVals.max()]\n",
    "yValsFit = [yInt[0, 1] + x*grad[0, 1] for x in xValsFit] # same mistake in the code!\n",
    "pyplot.plot(xVals, yVals, 'o')\n",
    "pyplot.plot(xValsFit, yValsFit)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa5210",
   "metadata": {},
   "source": [
    "## 2- Clustering and discrimination\n",
    "### 2-2- Clustering methods\n",
    "#### Simple threshold clustering\n",
    "This method groups items of data into clusters based upon how close (similar) the items of data are. The algorithm works by initially considering an item of data as a potentially separate cluster and then expands the cluster by adding any other items that are sufficiently close, i.e. the distance between data points is less than a threshold. Separable units of data are called **feature vectors**, referring to different kinds of measurement, e.g., weight, length, or height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate Euclidian distance between two items\n",
    "from numpy import dot, sqrt\n",
    "def euclideanDist(vectorA, vectorB):\n",
    "    diff = vectorA-vectorB\n",
    "    return sqrt(dot(diff,diff)) #square root of the sum of the squares of the values in the diff vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate distances of items and identify neighbours\n",
    "def findNeighbours(data, distFunc, threshold):\n",
    "    neighbourDict = {}\n",
    "    n = len(data)\n",
    "    for i in range(n):\n",
    "        neighbourDict[i] = []\n",
    "    for i in range(0,n-1):\n",
    "        for j in range(i+1,n):\n",
    "            dist = distFunc(data[i], data[j])\n",
    "            if dist < threshold:\n",
    "                neighbourDict[i].append(j)\n",
    "                neighbourDict[j].append(i)\n",
    "    return neighbourDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3355b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple clustering \n",
    "def simpleCluster(data, threshold, distFunc=euclideanDist):\n",
    "    neighbourDict = findNeighbours(data, distFunc, threshold)\n",
    "    clusters = []\n",
    "    pool = set(range(len(data)))\n",
    "    while pool:\n",
    "        i = pool.pop() #method removes the element at the specified position\n",
    "        neighbours = neighbourDict[i]\n",
    "        cluster = set()\n",
    "        cluster.add(i)\n",
    "        pool2 = set(neighbours)\n",
    "        while pool2:\n",
    "            j = pool2.pop()\n",
    "            if j in pool:\n",
    "                pool.remove(j)\n",
    "                cluster.add(j)\n",
    "                neighbours2 = neighbourDict[j]\n",
    "                pool2.update(neighbours2)\n",
    "        clusters.append(cluster)\n",
    "    clusterData = []\n",
    "    for cluster in clusters:\n",
    "        clusterData.append( [data[i] for i in cluster] )\n",
    "    return clusterData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52331857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random, vstack\n",
    "spread = 0.12\n",
    "sizeDims = (100,2)\n",
    "data = [random.normal(( 0.0, 0.0), spread, sizeDims), #centre location, standard deviation, size\n",
    "        random.normal(( 1.0, 1.0), spread, sizeDims),\n",
    "        random.normal(( 1.0, 0.0), spread, sizeDims)]\n",
    "data = vstack(data) # Stack arrays (concatenation)\n",
    "random.shuffle(data) # Randomise order\n",
    "clusters = simpleCluster(data, 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b53ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470397b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot clusters\n",
    "from matplotlib import pyplot\n",
    "colors = ['#F0F0F0','#A0A0A0','#505050', '#D0D0D0','#808080','#202020']\n",
    "markers = ['d','o','s','>','^']\n",
    "i = 0\n",
    "for cluster in clusters:\n",
    "    allX, allY = zip(*cluster)\n",
    "    if len(cluster) > 3:\n",
    "        color = colors[i % len(colors)]\n",
    "        marker = markers[i % len(markers)]\n",
    "        pyplot.scatter(allX, allY, s=30, c=color, marker=marker)\n",
    "        i += 1\n",
    "    else:\n",
    "        pyplot.scatter(allX, allY, s=5, c='black', marker='P')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f2701",
   "metadata": {},
   "source": [
    "#### Density-based clustering\n",
    "This method groups the data by taking into account the density of data points. The method here is referred to as DBSCAN: https://dl.acm.org/doi/10.5555/3001460.3001507."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b94a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbScanCluster(data, threshold, minNeighbour, distFunc=euclideanDist):\n",
    "    neighbourDict = findNeighbours(data, distFunc, threshold)\n",
    "    clusters = []\n",
    "    noise = set()\n",
    "    pool = set(range(len(data)))\n",
    "    while pool:\n",
    "        i = pool.pop()\n",
    "        neighbours = neighbourDict[i]\n",
    "        if len(neighbours) < minNeighbour:\n",
    "            noise.add(i)\n",
    "        else:\n",
    "            cluster = set()\n",
    "            cluster.add(i)\n",
    "            pool2 = set(neighbours)\n",
    "            while pool2:\n",
    "                j = pool2.pop()\n",
    "                if j in pool:\n",
    "                    pool.remove(j)\n",
    "                    neighbours2 = neighbourDict.get(j, [])\n",
    "                    if len(neighbours2) < minNeighbour:\n",
    "                        noise.add(j)\n",
    "                else:\n",
    "                    pool2.update(neighbours2)\n",
    "                    cluster.add(j)\n",
    "            clusters.append(cluster)\n",
    "    noiseData = [data[i] for i in noise]\n",
    "    clusterData = []\n",
    "    for cluster in clusters:\n",
    "        clusterData.append( [data[i] for i in cluster] )\n",
    "    return clusterData, noiseData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters, noise = dbScanCluster(data, 0.10, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49444afa",
   "metadata": {},
   "source": [
    "The above function took 3 hours, but no results - perhaps the code is wrong or something!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660c715",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "The '$k$' refers to the number of clusters that we wish to group our list data items into, and The 'means' refers to the concept of describing each cluster as the central average position, i.e., a geometric average of its members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, random\n",
    "from random import sample\n",
    "def kMeans(data, k, centers=None):\n",
    "    if centers is None:\n",
    "        centers = array(sample(list(data), k)) #randomly choose k items of the data\n",
    "    change = 1.0\n",
    "    while change > 1e-8:\n",
    "        clusters = [[] for x in range(k)]\n",
    "        for vector in data:\n",
    "            diffs = centers - vector\n",
    "            dists = (diffs * diffs).sum(axis=1)\n",
    "            closest = dists.argmin()\n",
    "            clusters[closest].append(vector)\n",
    "        change = 0\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            cluster = array(cluster)\n",
    "            center = cluster.sum(axis=0)/len(cluster)\n",
    "            diff = center - centers[i]\n",
    "            change += (diff * diff).sum()\n",
    "            centers[i] = center\n",
    "    return centers, clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65598ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example \n",
    "testDataA = random.random((1000,2))\n",
    "centers, clusters = kMeans(testDataA, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda0d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4515663e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a141b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clustering \n",
    "from matplotlib import pyplot\n",
    "colors = ['#FF0000','#00FF00','#0000FF','#FFFF00','#00FFFF','#FF00FF']\n",
    "for i, cluster in enumerate(clusters):\n",
    "    x, y = zip(*cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    pyplot.scatter(x, y, c=color, marker='o')\n",
    "x, y = zip(*centers)\n",
    "pyplot.scatter(x, y, s=40, c='black', marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataB1 = random.normal(0.0, 2.0, (100,2))\n",
    "testDataB2 = random.normal(7.0, 2.0, (100,2))\n",
    "testDataB = vstack([testDataB1, testDataB2]) # Two clumps\n",
    "centers, clusters = kMeans(testDataB, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969b8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d60c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clustering \n",
    "from matplotlib import pyplot\n",
    "colors = ['#FF0000','#00FF00','#0000FF','#FFFF00','#00FFFF','#FF00FF']\n",
    "for i, cluster in enumerate(clusters):\n",
    "    x, y = zip(*cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    pyplot.scatter(x, y, c=color, marker='o')\n",
    "x, y = zip(*centers)\n",
    "pyplot.scatter(x, y, s=40, c='black', marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4be5b5",
   "metadata": {},
   "source": [
    "#### Improving k-means\n",
    "The k-means algorithm can be improved by having a better guess at the starting cluster centres. For this, we guess one cluster centre by taking a random data point and then choose the centres of subsequent clusters by selecting points that are furthest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import zeros, ones, vstack\n",
    "from random import randint\n",
    "def kMeansSpread(data, k):\n",
    "    n = len(data)\n",
    "    index = randint(0, n-1)\n",
    "    indices = set([index])\n",
    "    influence = zeros(n)\n",
    "    while len(indices) < k:\n",
    "        diff = data - data[index]\n",
    "        sumSq = (diff * diff).sum(axis=1) + 1.0\n",
    "        influence += 1.0 / sumSq\n",
    "        index = influence.argmin()\n",
    "        while index in indices:\n",
    "            index = randint(0, n-1)\n",
    "        indices.add(index)\n",
    "    centers = vstack([data[i] for i in indices])\n",
    "    return kMeans(data, k, centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13520547",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers, clusters = kMeansSpread(testDataB, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21740d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clustering \n",
    "from matplotlib import pyplot\n",
    "colors = ['#FF0000','#00FF00','#0000FF','#FFFF00','#00FFFF','#FF00FF']\n",
    "for i, cluster in enumerate(clusters):\n",
    "    x, y = zip(*cluster)\n",
    "    color = colors[i % len(colors)]\n",
    "    pyplot.scatter(x, y, c=color, marker='o')\n",
    "x, y = zip(*centers)\n",
    "pyplot.scatter(x, y, s=40, c='black', marker='o')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737c11c",
   "metadata": {},
   "source": [
    "K-means clustering can be done with Scikit-Learn library. Details are available: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963cce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4).fit(testDataB)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab882c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clusters made with Scikit-Learn library \n",
    "import pandas as pd    \n",
    "import seaborn as sns\n",
    "testDataB_df = pd.DataFrame(testDataB, columns=['col_1', 'col_2'])\n",
    "testDataB_df['cluster'] = kmeans.labels_\n",
    "sns.scatterplot(data = testDataB_df, x = 'col_1', y = 'col_2', hue = 'cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5bd04",
   "metadata": {},
   "source": [
    "### 2-3- Data discrimination\n",
    "#### Principal component analysis\n",
    "Principal component analysis (PCA) gives the eigenvectors of the covariance matrix. Taking the eigenvalues of these in size order we can find the most significant [or] principal components that account for most of the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885461f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import cov, linalg, sqrt, zeros, ones, diag\n",
    "def principalComponentAnalysis(data, n):\n",
    "    samples, features = data.shape\n",
    "    meanVec = data.mean(axis=0)\n",
    "    dataC = (data - meanVec).T\n",
    "    covar = cov(dataC)\n",
    "    evals, evecs = linalg.eig(covar)\n",
    "    indices = evals.argsort()[::-1]\n",
    "    evecs = evecs[:,indices]\n",
    "    basis = evecs[:,:n] #the first n principal components\n",
    "    energy = evals[:n].sum() #a measure of how much covariance our top eigenvalues explain\n",
    "    # norm wrt to variance\n",
    "    #sd = sqrt(diag(covar))\n",
    "    #zscores = dataC.T / sd\n",
    "    return basis, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217d810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = random.normal(0.0, 2.0, (100,2))\n",
    "shear = array([[2,1],[1,0]])\n",
    "testData = dot(testData, shear)\n",
    "testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "basis, energy = principalComponentAnalysis(testData, 2)\n",
    "print('Full PCA:', basis, energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot original data (before transformation)\n",
    "from matplotlib import pyplot\n",
    "x,y = zip(*testData)\n",
    "pyplot.scatter(x, y, s=20, c='#F0F0F0', marker='o')\n",
    "x,y = zip(-10*basis, 10*basis)\n",
    "pyplot.plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30022983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot transformed data after applying PCA\n",
    "transformed = dot(testData, basis)\n",
    "x,y = zip(*transformed)\n",
    "pyplot.scatter(x, y, s=20, c='#000000', marker='^')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba4be9a",
   "metadata": {},
   "source": [
    "PCA can be done with Scikit-Learn library. Details are available: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "testData_pca = pca.fit_transform(testData)\n",
    "testData_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e88a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot transformed data (PCA with Scikit-Learn library) \n",
    "x,y = zip(*testData_pca)\n",
    "pyplot.scatter(x, y, s=20, c='#000000', marker='v')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ceb8d",
   "metadata": {},
   "source": [
    "## 3- Machine learning\n",
    "### 3-1- K-nearest neighbours\n",
    "A method to classify an unknown data vector by calculating the distance or similarity between points, i.e., comparing the unknown data to data vectors for which there is a known classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c6f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance calculation\n",
    "def getFeatureDistance(vector1, vector2):\n",
    "    distance = 0.0\n",
    "    for a, b in zip(vector1, vector2):\n",
    "        delta = a-b\n",
    "        distance += delta * delta\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27121d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking k closest neighbours\n",
    "def kNearestNeighbour(knowns, query, k=7):\n",
    "    if k >= len(knowns):\n",
    "        raise Exception('Length of training data must be larger than k')\n",
    "    dists = []\n",
    "    for vector, cat in knowns[:k]:\n",
    "        dist = getFeatureDistance(vector, query)\n",
    "        dists.append( (dist, cat) ) # Vector could be included\n",
    "    dists.sort()\n",
    "    closest = dists[:k]\n",
    "    counts = {}\n",
    "    for dist, cat in closest:\n",
    "        counts[cat] = counts.get(cat, 0) + 1\n",
    "    bestCount = max(counts.values())\n",
    "    bestCats = [cat for cat in counts if counts[cat] == bestCount]\n",
    "    for dist, cat in closest:\n",
    "        if cat in bestCats:\n",
    "            return cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b48328",
   "metadata": {},
   "source": [
    "#### Example\n",
    "A set of colour vectors are placed into two named categories, warm and cool, with equal number of each category to have unbiased prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ae98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knownClasses = [((1.0, 0.0, 0.0), 'warm'), # red\n",
    "                ((0.0, 1.0, 0.0), 'cool'), # green\n",
    "                ((0.0, 0.0, 1.0), 'cool'), # blue\n",
    "                ((0.0, 1.0, 1.0), 'cool'), # cyan\n",
    "                ((1.0, 1.0, 0.0), 'warm'), # yellow\n",
    "                ((1.0, 0.0, 1.0), 'warm'), # magenta\n",
    "                ((0.0, 0.0, 0.0), 'cool'), # black\n",
    "                ((0.5, 0.5, 0.5), 'cool'), # grey\n",
    "                ((1.0, 1.0, 1.0), 'cool'), # white\n",
    "                ((1.0, 1.0, 0.5), 'warm'), # light yellow\n",
    "                ((0.5, 0.0, 0.0), 'warm'), # maroon\n",
    "                ((1.0, 0.5, 0.5), 'warm'), # pink\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334351f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kNearestNeighbour(knownClasses, (0.7,0.7,0.2), k=3)\n",
    "print('Colour class:', result) # cool yellowish colour!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efaa4f",
   "metadata": {},
   "source": [
    "### 3-2- Feed-forward artificial neural networks\n",
    "The neural network is composed of a series of nodes arranged into three layers (input, middle or hidden, and output), but could have more layers! In feed-forward mechanism, input signals enter into the nodes of the first layer, then to those in the hidden layer, and finally move to those in the output layer. \n",
    "\n",
    "The number of input nodes represents the size of the input vector. The number of hidden nodes used will depend on the type and complexity but will normally be optimised to give the best predictions (between 3 and 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, tanh, zeros, ones, random, sum, append\n",
    "#prediction function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums) # hidden layer\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums) # output layer\n",
    "    return signalIn, signalHid, signalOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2):\n",
    "    #initialising numbers of nodes and minimum error value\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "    #making initial signal vectors as arrays of required sizes\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "    #initialising weight matrices with random values\n",
    "    wInp = random.random((numInp, numHid))-0.5\n",
    "    wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "    #initialising change matrices that indicate how much the weigh matrices differ from one training cycle to the next\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "    #initialising training data\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    "    #begin training\n",
    "    for step in range(steps): # xrange() in Python 2\n",
    "        random.shuffle(trainData) # Important\n",
    "        error = 0.0\n",
    "        #getting input feature vector in each for loop\n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)  \n",
    "            #apply back-propagation to reduce error\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff) \n",
    "            #adjustment to output layer\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff\n",
    "            #adjustment to hidden layer\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff\n",
    "            #update output\n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    "            #update input\n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    "        #check improvment of minimum error\n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f617ac3",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597434a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data\n",
    "data = [[[0,0], [0]],\n",
    "        [[0,1], [1]],\n",
    "        [[1,0], [1]],\n",
    "        [[1,1], [0]]]\n",
    "#training\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(data, 2, 1000)\n",
    "#predicting\n",
    "for inputs, knownOut in data:\n",
    "    sIn, sHid, sOut = neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print(knownOut, sOut[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb34424",
   "metadata": {},
   "source": [
    "Neural networks can be made with Scikit-Learn library. Details are available: https://scikit-learn.org/stable/modules/neural_networks_supervised.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e567bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "X = [[0, 0], [0,1], [1,0], [1, 1]]\n",
    "y = [0, 1, 1, 0]\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 4), random_state=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction of seen data\n",
    "preds = clf.predict_proba([[0, 0], [0, 1], [1,0], [1,1]])\n",
    "preds_1 = [item[1] for item in preds]\n",
    "print(preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b012f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction of unseen data\n",
    "preds = clf.predict_proba([[2, 2], [-1, -2]])\n",
    "preds_1 = [item[1] for item in preds]\n",
    "print(preds_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
